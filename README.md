# Local_ai_agent

# ğŸ§  CareerGuide AI â€” Local Career Counseling Assistant

**CareerGuide AI** is a privacy-first, AI-powered career guidance web application built with **Flask**, **LangChain**, and **Ollama (LLaMA 3.1)**. It runs **entirely on your local machine** without any internet-based AI API â€” perfect for offline environments.

---

## âœ¨ Features

- ğŸ’¬ **AI Chatbot** using locally hosted LLaMA 3.1 (via Ollama)
- ğŸ§  Contextual Q&A powered by LangChain
- ğŸ”„ Reset sessions and start new conversations easily

---

## ğŸ–¥ï¸ Live Preview

Once set up, visit: [http://localhost:5000](http://localhost:5000)

---

## ğŸ“ Project Structure

career-guide-ai/
â”œâ”€â”€ main.py # Flask application
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ templates/
  â””â”€â”€ index.html # Frontend HTML


---

## âš™ï¸ Setup Instructions

Follow these steps to run the project on your machine:

### 1. ğŸ§© Install Ollama

Download and install Ollama from the official website:  
ğŸ‘‰ [https://ollama.ai](https://ollama.ai)

### 2. ğŸ“¥ Pull the LLaMA 3.1 Model

Open your terminal and run:

```bash
ollama pull llama3.1

Then start the model:
ollama run llama3.1

Keep this running in a terminal tab or background process.

3. ğŸ“¦ Clone the Repository

git clone https://github.com/your-username/careerguide-ai.git
cd careerguide-ai

4. ğŸ§ª Create & Activate Virtual Environment
# Create virtual environment
python -m venv venv

# Activate it
# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate

5. ğŸ“¦ Install Dependencies
pip install -r requirements.txt

6. â–¶ï¸ Run the Application
python main.py
Visit: http://localhost:5000

ğŸ›  Technologies Used
Python 3.11+

Flask 2.3.3

LangChain (langchain-core, langchain-ollama)

Ollama + LLaMA 3.1

âœ… Requirements
Python 3.11 or higher

Ollama installed and LLaMA 3.1 pulled

Virtual environment with Flask and LangChain installed

ğŸ§° Troubleshooting
Problem	Solution
Ollama not responding	Ensure ollama run llama3.1 is running
Model not found	Re-run: ollama pull llama3.1
Port conflict	Change port in main.py (app.run(port=5001))
Session not working	Ensure SECRET_KEY is set in Flask app

ğŸ“¸ Screenshots
![image](https://github.com/user-attachments/assets/1a676f83-266e-4e2b-af69-e16336d76a5d)


ğŸ™ Acknowledgments
LangChain

Ollama

Meta LLaMA 3

